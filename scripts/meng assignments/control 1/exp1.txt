+------------------------------------------+
| Iter: 0                                  |
+------------------------------------------+
| Learning rate                            |
+------------------------------------------+
0.0002:	392.18387959508874 reward over 1501 timesteps
0.0003:	0.0 reward over 6001 timesteps
0.0004:	452.84065604328936 reward over 1501 timesteps
0.0005:	1835.501708984375 reward over 41 timesteps
0.0006:	569.7820434570312 reward over 30 timesteps
0.0007:	476.8611105159805 reward over 39 timesteps
0.0008:	469.01410578157726 reward over 18 timesteps
0.0009:	514.6567745835216 reward over 31 timesteps
0.0010:	0.0 reward over 6001 timesteps

Selected learning rate: 0.0005
+------------------------------------------+
| Entropy                                  |
+------------------------------------------+
1e-05:	0.0 reward over 6001 timesteps
5e-05:	344.8285425021897 reward over 3001 timesteps
0.0001:	672.5635375976562 reward over 4501 timesteps
0.0005:	513.0281372906261 reward over 12 timesteps

Selected entropy const: 0.0001
+------------------------------------------+
| Nstep for TD errors                      |
+------------------------------------------+
5:	529.3231302012958 reward over 17 timesteps
10:	1229.1368469658914 reward over 47 timesteps
20:	609.2783097145525 reward over 34 timesteps
inf:	3381.253662109375 reward over 338 timesteps

Selected nstep: inf
+------------------------------------------+
| Depth (Policy)                           |
+------------------------------------------+
2:	0.0 reward over 6001 timesteps
3:	0.0 reward over 6001 timesteps
4:	0.0 reward over 6001 timesteps

Selected Policy depth: 2
+------------------------------------------+
| Number of neurons in each layer (Policy) |
+------------------------------------------+
32:	0.0 reward over 6001 timesteps
64:	3451.6982421875 reward over 332 timesteps
128:	838.6336669921875 reward over 4501 timesteps

Selected Policy num neurons: 64
+------------------------------------------+
| Activation (Policy)                      |
+------------------------------------------+
relu:	615.5010986328125 reward over 3001 timesteps
leaky relu:	0.0 reward over 6001 timesteps
tanh:	0.0 reward over 6001 timesteps

Selected Policy activation: relu
+------------------------------------------+
| Depth (Value)                            |
+------------------------------------------+
2:	0.0 reward over 6001 timesteps
3:	0.0 reward over 6001 timesteps
4:	516.9172570064234 reward over 30 timesteps

Selected Value depth: 4
+------------------------------------------+
| Number of neurons in each layer (Value)  |
+------------------------------------------+
32:	743.4026692708334 reward over 1501 timesteps
64:	484.51568366745107 reward over 3001 timesteps
128:	0.0 reward over 6001 timesteps

Selected Value num neurons: 32
+------------------------------------------+
| Activation (Value)                       |
+------------------------------------------+
relu:	0.0 reward over 6001 timesteps
leaky relu:	0.0 reward over 6001 timesteps
tanh:	409.9370137854461 reward over 3001 timesteps

Selected Valiue activation: tanh
+------------------------------------------+
| State memory                             |
+------------------------------------------+
1:	0.0 reward over 6001 timesteps
5:	1817.12353515625 reward over 26 timesteps
10:	0.0 reward over 6001 timesteps

Selected state memory: 5
+------------------------------------------+
| Iter: 1                                  |
+------------------------------------------+
| Learning rate                            |
+------------------------------------------+
0.0002:	377.13429966029395 reward over 31 timesteps
0.0003:	550.0816932914028 reward over 26 timesteps
0.0004:	742.95654296875 reward over 4501 timesteps
0.0005:	838.7250366210938 reward over 4501 timesteps
0.0006:	0.0 reward over 6001 timesteps
0.0007:	603.6029470026148 reward over 4501 timesteps
0.0008:	0.0 reward over 6001 timesteps
0.0009:	0.0 reward over 6001 timesteps
0.0010:	0.0 reward over 6001 timesteps

Selected learning rate: 0.0005
+------------------------------------------+
| Entropy                                  |
+------------------------------------------+
1e-05:	686.2828369140625 reward over 3001 timesteps
5e-05:	0.0 reward over 6001 timesteps
0.0001:	766.8075561523438 reward over 4501 timesteps
0.0005:	0.0 reward over 6001 timesteps

Selected entropy const: 0.0001
+------------------------------------------+
| Nstep for TD errors                      |
+------------------------------------------+
5:	342.66555177383844 reward over 4501 timesteps
10:	603.738037109375 reward over 4501 timesteps
20:	2383.16552734375 reward over 252 timesteps
inf:	0.0 reward over 6001 timesteps

Selected nstep: 20
+------------------------------------------+
| Depth (Policy)                           |
+------------------------------------------+
2:	527.450018905675 reward over 3001 timesteps
3:	0.0 reward over 6001 timesteps
4:	0.0 reward over 6001 timesteps

Selected Policy depth: 2
+------------------------------------------+
| Number of neurons in each layer (Policy) |
+------------------------------------------+
32:	595.6113505427037 reward over 31 timesteps
64:	755.2738037109375 reward over 3001 timesteps
128:	0.0 reward over 6001 timesteps

Selected Policy num neurons: 64
+------------------------------------------+
| Activation (Policy)                      |
+------------------------------------------+
relu:	0.0 reward over 6001 timesteps
leaky relu:	684.3624877929688 reward over 3001 timesteps
tanh:	48547.359375 reward over 4841 timesteps

Selected Policy activation: tanh
+------------------------------------------+
| Depth (Value)                            |
+------------------------------------------+
2:	814.5001831054688 reward over 4501 timesteps
3:	2715.952880859375 reward over 216 timesteps
4:	2753.45556640625 reward over 198 timesteps

Selected Value depth: 4
+------------------------------------------+
| Number of neurons in each layer (Value)  |
+------------------------------------------+
32:	0.0 reward over 6001 timesteps
64:	3116.65625 reward over 285 timesteps
128:	0.0 reward over 6001 timesteps

Selected Value num neurons: 64
+------------------------------------------+
| Activation (Value)                       |
+------------------------------------------+
relu:	0.0 reward over 6001 timesteps
leaky relu:	0.0 reward over 6001 timesteps
tanh:	2837.32373046875 reward over 236 timesteps

Selected Valiue activation: tanh
+------------------------------------------+
| State memory                             |
+------------------------------------------+




Interrupted with ValueError :(
Restarted with hyperparameters set to optimal from last iteration:




+------------------------------------------+
| Iter: 0                                  |
+------------------------------------------+
| Learning rate                            |
+------------------------------------------+
0.0002: 2207.3766000711557 reward over 1501 timesteps
0.0003: 1444.3886433578953 reward over 28 timesteps
0.0004: 1463.4119021131414 reward over 16 timesteps
0.0005: 2855.876953125 reward over 249 timesteps
0.0006: 4123.3115234375 reward over 1501 timesteps
0.0007: 2745.06103515625 reward over 216 timesteps
0.0008: 2315.3322279351196 reward over 23 timesteps
0.0009: 4585.03759765625 reward over 1501 timesteps
0.0010: 4250.922029408346 reward over 1501 timesteps

Selected learning rate: 0.0009
+------------------------------------------+
| Entropy                                  |
+------------------------------------------+
1e-05:  3996.7774041718276 reward over 1501 timesteps
5e-05:  3996.27869389591 reward over 1501 timesteps
0.0001: 3996.3915871453673 reward over 1501 timesteps
0.0005: 2731.50439453125 reward over 227 timesteps

Selected entropy const: 1e-05
+------------------------------------------+
| Nstep for TD errors                      |
+------------------------------------------+
5:      4309.221735414165 reward over 26 timesteps
10:     2746.046142578125 reward over 227 timesteps
20:     2326.9373346999864 reward over 240 timesteps
inf:    3996.478759765625 reward over 1501 timesteps

Selected nstep: 5
+------------------------------------------+
| Depth (Policy)                           |
+------------------------------------------+
2:      15464.07421875 reward over 1501 timesteps
3:      2754.576904296875 reward over 241 timesteps
4:      6969.484375 reward over 1501 timesteps

Selected Policy depth: 2
+------------------------------------------+
| Number of neurons in each layer (Policy) |
+------------------------------------------+
32:     4722.549265120889 reward over 1501 timesteps
64:     3996.3083215132415 reward over 1501 timesteps
128:    2176.0306940940304 reward over 24 timesteps

Selected Policy num neurons: 32
+------------------------------------------+
| Activation (Policy)                      |
+------------------------------------------+
relu:   1470.6222758150159 reward over 25 timesteps
leaky relu:     1551.299663479599 reward over 33 timesteps
tanh:   1346.7867299573354 reward over 13 timesteps

Selected Policy activation: leaky relu
+------------------------------------------+
| Depth (Value)                            |
+------------------------------------------+
2:      2948.678466796875 reward over 239 timesteps
3:      483.10554872930874 reward over 25 timesteps
4:      1430.3478631888838 reward over 1501 timesteps

Selected Value depth: 2
+------------------------------------------+
| Number of neurons in each layer (Value)  |
+------------------------------------------+
32:     1369.8574106306203 reward over 37 timesteps
64:     1612.3372802734375 reward over 44 timesteps
128:    1468.364013671875 reward over 1501 timesteps

Selected Value num neurons: 64
+------------------------------------------+
| Activation (Value)                       |
+------------------------------------------+
relu:   1435.7707913046597 reward over 29 timesteps
leaky relu:     1799.308073116616 reward over 15 timesteps
tanh:   634.430419921875 reward over 26 timesteps

Selected Valiue activation: leaky relu
+------------------------------------------+
| State memory                             |
+------------------------------------------+
1:      1625.2215295573142 reward over 247 timesteps
5:      2412.2060546875 reward over 251 timesteps
10:     3099.6134019298793 reward over 1501 timesteps

Selected state memory: 10
